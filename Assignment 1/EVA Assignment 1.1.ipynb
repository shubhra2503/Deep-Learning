{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:block\">\n",
    "    <div style=\"width: 100%; display: inline-block\">\n",
    "        <h5  style=\"color:maroon; text-align: left; font-size:25px;\">EVA Assignment: 1.1</h5>\n",
    "        <div style=\"width: 100%; text-align: left; display: inline-block;\"><i>Author(s): </i> <strong>Shubhra Prakash</strong> </div>\n",
    "               <div style=\"width: 100%; text-align: left; display: inline-block;\"><i>Modified: July 13th, 2020</i> </div>\n",
    "    </div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Channels and Kernels (according to EVA)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Kernels </strong>\n",
    "Kernels-are the unique 2D array of weights which convolves onto an input channel of the previous layer to generate one output channel.Each kernel focuses on specific aspects of the input channel in an image.The kernels can also be thought of as feature extractors\n",
    "\n",
    "<strong> Filters </strong>\n",
    "A filter is a collection of kernels.\n",
    "\n",
    "<strong> Channels</strong>\n",
    "Channel is a collection of similar features.It can be thought of as a feature map generated by a kernel after the convolution.Channels are the building blocks of a layer in DNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Source/1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source [Link](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Why should we (nearly) always use 3x3 kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <strong>Axis of symmetry</strong>- odd numbered kernels have axis of symmetry\n",
    "- <strong>Usage</strong>- 3X3 matrix can be used to form any other odd numbered matrix\n",
    "- <strong>GPU</strong>-Most of the GPU manufacturers have accelarated usage of 3X3 for their products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>We need to perform 99 (3x3) convolutions operations to reach close to 1x1 from 199x199 </strong>\n",
    "\n",
    "<strong>Calculations</strong>\n",
    "\n",
    "199X199<|3X3|>197X197 \n",
    "\n",
    "197X197<|3X3|>195X195 \n",
    "\n",
    "195X195<|3X3|>193X193 \n",
    "\n",
    "193X193<|3X3|>191X191 \n",
    "\n",
    "191X191<|3X3|>189X189 \n",
    "\n",
    "189X189<|3X3|>187X187 \n",
    "\n",
    "187X187<|3X3|>185X185 \n",
    "\n",
    "185X185<|3X3|>183X183 \n",
    "\n",
    "183X183<|3X3|>181X181 \n",
    "\n",
    "181X181<|3X3|>179X179 \n",
    "\n",
    "179X179<|3X3|>177X177 \n",
    "\n",
    "177X177<|3X3|>175X175 \n",
    "\n",
    "175X175<|3X3|>173X173 \n",
    "\n",
    "173X173<|3X3|>171X171 \n",
    "\n",
    "171X171<|3X3|>169X169 \n",
    "\n",
    "169X169<|3X3|>167X167 \n",
    "\n",
    "167X167<|3X3|>165X165 \n",
    "\n",
    "165X165<|3X3|>163X163 \n",
    "\n",
    "163X163<|3X3|>161X161 \n",
    "\n",
    "161X161<|3X3|>159X159 \n",
    "\n",
    "159X159<|3X3|>157X157 \n",
    "\n",
    "157X157<|3X3|>155X155 \n",
    "\n",
    "155X155<|3X3|>153X153 \n",
    "\n",
    "153X153<|3X3|>151X151 \n",
    "\n",
    "151X151<|3X3|>149X149 \n",
    "\n",
    "149X149<|3X3|>147X147 \n",
    "\n",
    "147X147<|3X3|>145X145 \n",
    "\n",
    "145X145<|3X3|>143X143 \n",
    "\n",
    "143X143<|3X3|>141X141 \n",
    "\n",
    "141X141<|3X3|>139X139 \n",
    "\n",
    "139X139<|3X3|>137X137 \n",
    "\n",
    "137X137<|3X3|>135X135 \n",
    "\n",
    "135X135<|3X3|>133X133 \n",
    "\n",
    "133X133<|3X3|>131X131 \n",
    "\n",
    "131X131<|3X3|>129X129 \n",
    "\n",
    "129X129<|3X3|>127X127 \n",
    "\n",
    "127X127<|3X3|>125X125 \n",
    "\n",
    "125X125<|3X3|>123X123 \n",
    "\n",
    "123X123<|3X3|>121X121 \n",
    "\n",
    "121X121<|3X3|>119X119 \n",
    "\n",
    "119X119<|3X3|>117X117 \n",
    "\n",
    "117X117<|3X3|>115X115 \n",
    "\n",
    "115X115<|3X3|>113X113 \n",
    "\n",
    "113X113<|3X3|>111X111 \n",
    "\n",
    "111X111<|3X3|>109X109 \n",
    "\n",
    "109X109<|3X3|>107X107 \n",
    "\n",
    "107X107<|3X3|>105X105 \n",
    "\n",
    "105X105<|3X3|>103X103 \n",
    "\n",
    "103X103<|3X3|>101X101 \n",
    "\n",
    "101X101<|3X3|>99X99 \n",
    "\n",
    "99X99<|3X3|>97X97 \n",
    "\n",
    "97X97<|3X3|>95X95 \n",
    "\n",
    "95X95<|3X3|>93X93 \n",
    "\n",
    "93X93<|3X3|>91X91 \n",
    "\n",
    "91X91<|3X3|>89X89 \n",
    "\n",
    "89X89<|3X3|>87X87 \n",
    "\n",
    "87X87<|3X3|>85X85 \n",
    "\n",
    "85X85<|3X3|>83X83 \n",
    "\n",
    "83X83<|3X3|>81X81 \n",
    "\n",
    "81X81<|3X3|>79X79 \n",
    "\n",
    "79X79<|3X3|>77X77 \n",
    "\n",
    "77X77<|3X3|>75X75 \n",
    "\n",
    "75X75<|3X3|>73X73 \n",
    "\n",
    "73X73<|3X3|>71X71 \n",
    "\n",
    "71X71<|3X3|>69X69 \n",
    "\n",
    "69X69<|3X3|>67X67 \n",
    "\n",
    "67X67<|3X3|>65X65 \n",
    "\n",
    "65X65<|3X3|>63X63 \n",
    "\n",
    "63X63<|3X3|>61X61 \n",
    "\n",
    "61X61<|3X3|>59X59 \n",
    "\n",
    "59X59<|3X3|>57X57 \n",
    "\n",
    "57X57<|3X3|>55X55 \n",
    "\n",
    "55X55<|3X3|>53X53 \n",
    "\n",
    "53X53<|3X3|>51X51 \n",
    "\n",
    "51X51<|3X3|>49X49 \n",
    "\n",
    "49X49<|3X3|>47X47 \n",
    "\n",
    "47X47<|3X3|>45X45 \n",
    "\n",
    "45X45<|3X3|>43X43 \n",
    "\n",
    "43X43<|3X3|>41X41 \n",
    "\n",
    "41X41<|3X3|>39X39 \n",
    "\n",
    "39X39<|3X3|>37X37 \n",
    "\n",
    "37X37<|3X3|>35X35 \n",
    "\n",
    "35X35<|3X3|>33X33 \n",
    "\n",
    "33X33<|3X3|>31X31 \n",
    "\n",
    "31X31<|3X3|>29X29 \n",
    "\n",
    "29X29<|3X3|>27X27 \n",
    "\n",
    "27X27<|3X3|>25X25 \n",
    "\n",
    "25X25<|3X3|>23X23 \n",
    "\n",
    "23X23<|3X3|>21X21 \n",
    "\n",
    "21X21<|3X3|>19X19 \n",
    "\n",
    "19X19<|3X3|>17X17 \n",
    "\n",
    "17X17<|3X3|>15X15 \n",
    "\n",
    "15X15<|3X3|>13X13 \n",
    "\n",
    "13X13<|3X3|>11X11 \n",
    "\n",
    "11X11<|3X3|>9X9 \n",
    "\n",
    "9X9<|3X3|>7X7 \n",
    "\n",
    "7X7<|3X3|>5X5 \n",
    "\n",
    "5X5<|3X3|>3X3 \n",
    "\n",
    "3X3<|3X3|>1X1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are kernels initialized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The kernels are initialized at a small random value so that a stochastic optimization algorithm can be used to minimize the cost function**\n",
    "\n",
    "At the starting of the problem, we do not have an understanding of the structure of the search space.Training algorithms for deep learning models are usually iterative in nature and thus require the user to specify some initial point from which to begin the iterations.Therefore, to remove bias from the search process, we start from a randomly chosen position. While optimizing the cost function the optimization algorithm might get stuck in a local optima, randomness during the search process increases the probability of getting unstuck, provides multiple opportunities to start and traverse the space and find a better optimal solution.\n",
    "\n",
    "**Why not initialize with same values?(Zeros or ones)**\n",
    "\n",
    "Usually multiple perceptrons with the same activation function are connected to the same inputs, if they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way.This will make it difficult to break symmetry here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Source/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source [Link](https://machinelearningmastery.com/why-training-a-neural-network-is-hard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens during the training of a DNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Model Life-Cycle involves following steps:\n",
    "\n",
    "* Step 1: Prepare the Data\n",
    "* Step 2: Define the Model\n",
    "* Step 3: Train the Model\n",
    "* Step 4: Evaluate the Model\n",
    "* Step 5: Make Predictions\n",
    "\n",
    "Training of the DNN involves increase the accuracy by iterating over a training data set while tweaking the parameters(the weights and biases) of the model.The training mainly revolves around decreasing the prediction error or **cost**. By minimizing the loss with respect to the network parameters, we can find a state where the loss is at a minimum and the network is able to predict the correct labels at a high accuracy. We find this minimum loss using a process called **gradient descent**.The aim of the algorithm is getting to the lowest error value. The gradient is the slope of the loss function and points in the direction of fastest change until we get to the bottom of our graph, or to a point where we can no longer move downhill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "* [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215#:~:text=A%20layer%20could%20have%20multiple,%E2%80%9D%20(%E2%80%9Ckernel%E2%80%9D).)\n",
    "* [Why Initialize a Neural Network with Random Weights?](https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/)\n",
    "* [Weight Initialization Techniques in Neural Networks](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
